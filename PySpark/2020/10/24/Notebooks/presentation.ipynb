{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python",
      "version": "3.7.3-final"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speaker\n",
        "\n",
        "|  |  |\n",
        "| :-- | :-- |\n",
        "| <img src=\"https://avatars1.githubusercontent.com/u/46888598?s=460&u=1832ff2628bf03f8a7e2b1d672e6af340fd05eff&v=4\" width=\"100px\"> | **Yuji Masaoka**<br><br>青い R の中の人。<br><br>Twitter: [\b@mappie_kochi](https://twitter.com/mappie_kochi) <br>GitHub: [ymasaoka](https://github.com/ymasaoka)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Intro\n",
        "\n",
        "October 24th, 2020  \n",
        "第 35 回 SQL Server 2019 勉強会\n",
        "\n",
        "Japan SQL Server User Group - PASS Local Group"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark とは\n",
        "\n",
        "<h2>Spark を実行するための Python API</h2>  \n",
        "Python プログラミングで Apache Spark を利用できる。<br>\n",
        "PyPI など Python ライブラリを組み込むことも可能。\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- browser: string (nullable = true)\n",
            " |-- OS: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n",
            "+-------+-------+-------+-----+---+\n",
            "|user_id|country|browser|   OS|age|\n",
            "+-------+-------+-------+-----+---+\n",
            "|   A203|  India| Chrome|  WIN| 33|\n",
            "|   A201|  China| Safari|MacOS| 35|\n",
            "|   A205|     UK|Mozilla|Linux| 25|\n",
            "+-------+-------+-------+-----+---+\n",
            "\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('data_processing').getOrCreate()\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType().add(\"user_id\",\"string\").add(\"country\",\"string\").add(\"browser\", \"string\").add(\"OS\",'string').add(\"age\", \"integer\")\n",
        "df = spark.createDataFrame([(\"A203\",'India',\"Chrome\",\"WIN\",33),(\"A201\",'China',\"Safari\",\"MacOS\",35),(\"A205\",'UK',\"Mozilla\",\"Linux\",25)], schema = schema)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apache Spark とは\n",
        "\n",
        "さすが Microsoft さん。\n",
        "\n",
        "- [Azure Synapse Analytics での Apache Spark](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/spark/apache-spark-overview)\n",
        "\n",
        "> Apache Spark は、ビッグデータ分析アプリケーションのパフォーマンスを向上させるメモリ内処理をサポートする並列処理フレームワークです。\n",
        "\n",
        "つまり、\n",
        "\n",
        "ビックデータなど、大きいデータに対して、高速に、インメモリで、分散処理を行うことができるオープンソースのフレームワーク = Apache Spark。  \n",
        "Java や Scala、Python などいろいろな API が用意されている。  \n",
        "分散処理のややこしい部分をうまく抽象化してくれているため、ユーザーは簡潔なコードを実行するだけで OK。  \n",
        "また、以下のような便利なコンポーネントが付属している。  \n",
        "\n",
        "- **Spark SQL**: クラスタ上のデータを SQL で処理\n",
        "- **Spark R、MLlib**: 機械学習\n",
        "- **Spark GraphX**: グラフ処理\n",
        "- **Spark Streaming**: ストリーミング処理\n",
        "\n",
        "詳しくは MS 畠山さんの [Azure Synapse Analytics Spark Pool](https://sqlserver.connpass.com/event/186147/) セッションで解説があった(はず)なので、ここでは割愛します。\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark 環境構築\n",
        "\n",
        "簡単に PySpark をサクっと始めたい場合は、以下を行うと良いと思います。(個人的意見)\n",
        "\n",
        "- **Azure Synapse Analytics で行う場合**: Spark プールを作りましょう\n",
        "- **ローカル環境で行う場合**: Spark インストールしましょう (Docker で用意するのが楽)\n",
        "\n",
        "```yaml:docker-compose.yaml\n",
        "version: '3'\n",
        "\n",
        "services:\n",
        "  notebook:\n",
        "    image: jupyter/pyspark-notebook\n",
        "    container_name: pyspark-notebook\n",
        "    ports:\n",
        "      - 8888:8888\n",
        "    volumes:\n",
        "      - ./src/pyspark-notebook:/home/jovyan/work\n",
        "    environment:\n",
        "      - NB_USER=jovyan\n",
        "      - GRANT_SUDO=yes\n",
        "      - user=root\n",
        "    command: start-notebook.sh\n",
        "```\n"
      ],
      "attachments": {}
    },
    {
      "source": [
        "# PySpark を使用したデータ処理 (入門バージョン)\n",
        "\n",
        "セッションの内容は、**「(SQL でやっている XXX って) PySpark ってこうやってデータの処理やるんだな」**といった感じで、`皆さんの頭の中でいろいろ考えを巡らせながら`聞いていただけると嬉しいです。\n",
        "\n",
        "今日ご紹介する内容は、大きく分けてこんな感じです。\n",
        "\n",
        "- Dataframes (データフレーム)\n",
        "- Select (選択)\n",
        "- Filter (フィルタリング)\n",
        "- Where (条件指定)\n",
        "- Aggregations (集合)\n",
        "- Collect (配列)\n",
        "- User-Defined Functions (ユーザー定義関数)\n",
        "- Joins (結合)\n",
        "- Pivoting (ピボット)\n",
        "- Window Functions or Window Aggregates (ウインドウ関数またはウインドウ集計)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Dataframes (データフレーム)\n",
        "\n",
        "いわゆる、リレーショナルデータベース (RDB) でいうところの`テーブル`のようなものです。(ざっくり)  \n",
        "Python の世界だと、pandas.DataFrame とかもありますが、Spark の世界では v1.3 より `Spark DataFrame` という機能が追加されており、今回はこちらを使用します。\n",
        "\n",
        "Dataframe のいいところは、以下の通りです。\n",
        "\n",
        "- SQL に近い文法で、Where や Join などができる\n",
        "- select や filter を使って、条件に該当する行や列を抜き出せる\n",
        "- 独自のユーザー定義関数 (User-Defined Functions: UDF) が利用できる\n",
        "- 集計も可能"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### SparkSession オブジェクトの作成\n",
        "\n",
        "Spark Dataframe の使用を始めるためには、まず `SparkSession オブジェクト` を作成する必要があります。  \n",
        "また、データフレームで列に設定するデータ型、PySpark 実行に必要な関数をインポートしておきます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SparkSession オブジェクトを作成する\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('data_processing').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データ型や実行に必要な関数をインポート\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "source": [
        "`StructType().add()` で二次元表のスキーマ(列)を指定していきます。  \n",
        ".add() をつなげて宣言することで、列を追加できます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# スキーマを宣言して、スキーマオブジェクトを作成\n",
        "schema = StructType().add(\"user_id\",\"string\").add(\"country\",\"string\").add(\"browser\", \"string\").add(\"OS\",'string').add(\"age\", \"integer\")"
      ]
    },
    {
      "source": [
        "上で作成したスキーマオブジェクトを `spark.createDataFrame()` に渡してあげる形で、データフレームを作成していきます。  \n",
        "この時に、初期データを一緒に設定することも可能です。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データフレームを作成\n",
        "df = spark.createDataFrame([(\"A203\",'India',\"Chrome\",\"WIN\",33),(\"A201\",'China',\"Safari\",\"MacOS\",35),(\"A205\",'UK',\"Mozilla\",\"Linux\",25)], schema = schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- user_id: string (nullable = true)\n |-- country: string (nullable = true)\n |-- browser: string (nullable = true)\n |-- OS: string (nullable = true)\n |-- age: integer (nullable = true)\n\n"
          ]
        }
      ],
      "source": [
        "# データフレームのスキーマ情報を確認\n",
        "df.printSchema()"
      ]
    },
    {
      "source": [
        "データフレームの中身を確認する (RDB でいうところの SELECT) には、`show()` を使用します。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+-----+---+\n|user_id|country|browser|   OS|age|\n+-------+-------+-------+-----+---+\n|   A203|  India| Chrome|  WIN| 33|\n|   A201|  China| Safari|MacOS| 35|\n|   A205|     UK|Mozilla|Linux| 25|\n+-------+-------+-------+-----+---+\n\n"
          ]
        }
      ],
      "source": [
        "# データフレームの中身を確認\n",
        "df.show()"
      ]
    },
    {
      "source": [
        "また、読み込んだデータの値を一部、置き換えることも可能です。  \n",
        "値を置き換えるには、`replace()` を使用します。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------+-----+---+\n|user_id|country|      browser|   OS|age|\n+-------+-------+-------------+-----+---+\n|   A203|  India|Google Chrome|  WIN| 33|\n|   A201|  China|       Safari|MacOS| 35|\n|   A205|     UK|      Mozilla|Linux| 25|\n+-------+-------+-------------+-----+---+\n\n"
          ]
        }
      ],
      "source": [
        "# Chrome を Google Chrome に置き換え\n",
        "df.replace(\"Chrome\", \"Google Chrome\").show()"
      ]
    },
    {
      "source": [
        "なお、このデータフレームは CSV ファイルなどを直接読み込んで作成することも可能です。  \n",
        "CSV からデータフレームを作成する方法はいくつか存在しますが (ex. Spark RDD 経由でインポート)、spark-csv を使用してみます。\n",
        "セッション時間の都合上、今回は割愛しますが、同様に JSON データを Spark で読み込むことも可能です。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# CSV ファイルをデータフレームとして読み込む\n",
        "df = spark.read.csv(\"customer_data.csv\", header=True, inferSchema=True)\n",
        "df.show(10)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Path does not exist: file:/home/jovyan/customer_data.csv;",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f40d3aaa169a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CSV ファイルをデータフレームとして読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/customer_data.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0|\n|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0|\n|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0|\n|Modern, complete ...|               1|                 3|40-50 years|      Average Family|     19504|    0|\n|  Large family farms|               1|                 4|30-40 years|             Farmers|     34943|    0|\n|    Young and rising|               1|                 2|20-30 years|         Living well|     13064|    0|\n|Large religious f...|               2|                 3|30-40 years|Conservative fami...|     29090|    0|\n|Lower class large...|               1|                 2|40-50 years|Family with grown...|      6895|    0|\n|Lower class large...|               1|                 2|50-60 years|Family with grown...|     35497|    0|\n|     Family starters|               2|                 3|40-50 years|      Average Family|     30800|    0|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\nonly showing top 10 rows\n\n"
          ]
        }
      ],
      "source": [
        "# Docker コンテナで動かす場合は、volumes マウント側に CSV を用意して、コンテナ内のパスを指定して実行\n",
        "df = spark.read.csv(\"./work/Chapter 2/customer_data.csv\", header=True, inferSchema=True)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV ファイルをデータフレームとして読み込む (Synapse Analytics バージョン)\n",
        "df = spark.read.load('abfss://<your ABFSS Path>', format='csv', header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(10)"
      ]
    },
    {
      "source": [
        "### Null 値への対応\n",
        "\n",
        "データには、Null 値が存在している場合もあると思います。  \n",
        "PySpark では、`fillna()` を使って Null 値を書き換えたり、`drop()` を使ってデータを省くことが可能です。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# null 値でを含んだデータフレームを作成 \n",
        "df_na = spark.createDataFrame([(\"A203\",None,\"Chrome\",\"WIN\",33),(\"A201\",'China',None,\"MacOS\",35),(\"A205\",'UK',\"Mozilla\",\"Linux\",25)], schema = schema)\n",
        "df_na.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# すべての Null 値を 0 (文字列)で埋める\n",
        "df_na.fillna('0').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 列ごとに指定の値で埋める\n",
        "df_na.fillna({ 'country':'USA', 'browser':'Safari' }).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Null を含む行を省いてみる - その 1 \n",
        "df_na.na.drop().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Null を含む行を省いてみる - その 2\n",
        "df_na.na.drop(subset = 'country').show()"
      ]
    },
    {
      "source": [
        "`drop()` では、列を削除することもできます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+---+\n|country|browser|   OS|age|\n+-------+-------+-----+---+\n|   null| Chrome|  WIN| 33|\n|  China|   null|MacOS| 35|\n|     UK|Mozilla|Linux| 25|\n+-------+-------+-----+---+\n\n"
          ]
        }
      ],
      "source": [
        "# user_id 列を削除\n",
        "df_na.drop('user_id').show()"
      ]
    },
    {
      "source": [
        "## Select (選択)\n",
        "\n",
        "RDB で言う `SELECT 句` みたいなものです。  \n",
        "注意するとすれば、**PySpark では show() を打たないとデータは確認できない** という点ぐらいです。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0|\n|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0|\n|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0|\n|Modern, complete ...|               1|                 3|40-50 years|      Average Family|     19504|    0|\n|  Large family farms|               1|                 4|30-40 years|             Farmers|     34943|    0|\n|    Young and rising|               1|                 2|20-30 years|         Living well|     13064|    0|\n|Large religious f...|               2|                 3|30-40 years|Conservative fami...|     29090|    0|\n|Lower class large...|               1|                 2|40-50 years|Family with grown...|      6895|    0|\n|Lower class large...|               1|                 2|50-60 years|Family with grown...|     35497|    0|\n|     Family starters|               2|                 3|40-50 years|      Average Family|     30800|    0|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\nonly showing top 10 rows\n\n"
          ]
        }
      ],
      "source": [
        "# 列を選択しないでデータ取得\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n|    Customer_subtype|Avg_Salary|\n+--------------------+----------+\n|Lower class large...|     44905|\n|Mixed small town ...|     37575|\n|Mixed small town ...|     27915|\n|Modern, complete ...|     19504|\n|  Large family farms|     34943|\n|    Young and rising|     13064|\n|Large religious f...|     29090|\n|Lower class large...|      6895|\n|Lower class large...|     35497|\n|     Family starters|     30800|\n+--------------------+----------+\nonly showing top 10 rows\n\n"
          ]
        }
      ],
      "source": [
        "# 列を選択してデータ取得\n",
        "df.select(['Customer_subtype','Avg_Salary']).show(10)"
      ]
    },
    {
      "source": [
        "## Filter (フィルタリング)\n",
        "\n",
        "RDB でいう `WHERE` 句の **(ただし AND のみ)** に該当するようなものです。  \n",
        "`filter()` を使ってデータフレームを AND 条件でフィルタリングすることができます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# filter() でデータを指定 - その 1\n",
        "df.filter(df['Avg_Salary'] > 1000000).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------+----------+\n|    Customer_subtype|Number_of_houses|    Avg_age|Avg_Salary|\n+--------------------+----------------+-----------+----------+\n|Affluent senior a...|               3|50-60 years|    596723|\n|Affluent senior a...|               3|50-60 years|    944444|\n|Affluent senior a...|               3|50-60 years|    788477|\n|Affluent senior a...|               3|50-60 years|    994077|\n+--------------------+----------------+-----------+----------+\n\n"
          ]
        }
      ],
      "source": [
        "# filter() でデータを指定 - その 2\n",
        "df.select(['Customer_subtype','Number_of_houses','Avg_age','Avg_Salary']).filter(df['Avg_Salary'] > 500000).filter(df['Number_of_houses'] > 2).show()"
      ]
    },
    {
      "source": [
        "## Where (条件指定)\n",
        "\n",
        "先ほどの **Filter** と同じく、RDB でいう `WHERE` 句に該当するようなものです。  \n",
        "Filter と違う点は `オペランドが利用可能` という点です。  \n",
        "`where()` の中で、条件を指定していきます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    596723|    0|\n|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    944444|    0|\n|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    788477|    0|\n|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    994077|    0|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n\n"
          ]
        }
      ],
      "source": [
        "# オペランドを利用 - その 1\n",
        "df.where((df['Avg_Salary'] > 500000) & (df['Number_of_houses'] > 2)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n| High status seniors|               1|                 3|40-50 years|Successful hedonists|   4670288|    0|\n|Affluent young fa...|               1|                 3|30-40 years|      Average Family|    762769|    1|\n| High status seniors|               1|                 3|50-60 years|Successful hedonists|   9561873|    0|\n| High status seniors|               1|                 2|40-50 years|Successful hedonists|  18687005|    0|\n| High status seniors|               1|                 2|40-50 years|Successful hedonists|  24139960|    0|\n| High status seniors|               1|                 2|50-60 years|Successful hedonists|   6718606|    0|\n|High Income, expe...|               1|                 3|40-50 years|Successful hedonists|  19347139|    0|\n|High Income, expe...|               1|                 3|40-50 years|Successful hedonists|   5243902|    0|\n| High status seniors|               2|                 3|40-50 years|Successful hedonists|   6138618|    0|\n|High Income, expe...|               1|                 3|50-60 years|Successful hedonists|  24930053|    0|\n|Mixed apartment d...|               3|                 2|50-60 years|         Living well|     30321|    0|\n| High status seniors|               1|                 2|50-60 years|Successful hedonists|  12545905|    1|\n|High Income, expe...|               1|                 3|40-50 years|Successful hedonists|  29976435|    0|\n| High status seniors|               1|                 2|50-60 years|Successful hedonists|  24639614|    0|\n| High status seniors|               1|                 2|40-50 years|Successful hedonists|  16073966|    0|\n|Affluent senior a...|               2|                 2|50-60 years|Successful hedonists|    690080|    0|\n|High Income, expe...|               1|                 4|40-50 years|Successful hedonists|  35032441|    1|\n|Affluent senior a...|               1|                 1|40-50 years|Successful hedonists|    536343|    0|\n|    Own home elderly|               3|                 2|50-60 years|    Cruising Seniors|     22252|    0|\n|High Income, expe...|               1|                 2|50-60 years|Successful hedonists|   8354410|    0|\n+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# オペランドを利用 - その 2\n",
        "df.where((df['Avg_Salary'] > 500000) | (df['Number_of_houses'] > 2)).show()"
      ]
    },
    {
      "source": [
        "## Aggregations (集合)\n",
        "\n",
        "RDB でいう `GROUP BY` 句まわりに該当する内容です。  \n",
        "`groupBy()` を使用します。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n|    Customer_subtype|count|\n+--------------------+-----+\n|Large family, emp...|   56|\n|Religious elderly...|   47|\n|Large religious f...|  107|\n|Modern, complete ...|   93|\n|    Village families|   68|\n|Young all america...|   62|\n|Young urban have-...|    4|\n|Young seniors in ...|   22|\n|Fresh masters in ...|    2|\n|High Income, expe...|   52|\n|Lower class large...|  288|\n| Residential elderly|    6|\n|Senior cosmopolitans|    1|\n|        Mixed rurals|   67|\n|Career and childcare|   33|\n|Low income catholics|   72|\n|Mixed apartment d...|   34|\n|Seniors in apartm...|   17|\n|Middle class fami...|  122|\n|Traditional families|  129|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_subtype で GROUP BY して COUNT(*)\n",
        "df.groupBy('Customer_subtype').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " *** Customer_subtype の集計結果 ***\n",
            "+------------------------------------------+-----+\n",
            "|Customer_subtype                          |count|\n",
            "+------------------------------------------+-----+\n",
            "|Lower class large families                |288  |\n",
            "|Traditional families                      |129  |\n",
            "|Middle class families                     |122  |\n",
            "|Large religious families                  |107  |\n",
            "|Modern, complete families                 |93   |\n",
            "|Couples with teens 'Married with children'|83   |\n",
            "|Young and rising                          |78   |\n",
            "|High status seniors                       |76   |\n",
            "|Low income catholics                      |72   |\n",
            "|Mixed seniors                             |71   |\n",
            "|Village families                          |68   |\n",
            "|Mixed rurals                              |67   |\n",
            "|Stable family                             |62   |\n",
            "|Young all american family                 |62   |\n",
            "|Young, low educated                       |56   |\n",
            "|Large family, employed child              |56   |\n",
            "|Family starters                           |55   |\n",
            "|High Income, expensive child              |52   |\n",
            "|Mixed small town dwellers                 |47   |\n",
            "|Religious elderly singles                 |47   |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            " *** Number_of_houses の集計結果 ***\n",
            "+----------------+-----+\n",
            "|Number_of_houses|count|\n",
            "+----------------+-----+\n",
            "|1               |1808 |\n",
            "|2               |178  |\n",
            "|3               |12   |\n",
            "|5               |1    |\n",
            "|10              |1    |\n",
            "+----------------+-----+\n",
            "\n",
            " *** Avg_size_household の集計結果 ***\n",
            "+------------------+-----+\n",
            "|Avg_size_household|count|\n",
            "+------------------+-----+\n",
            "|3                 |900  |\n",
            "|2                 |730  |\n",
            "|4                 |255  |\n",
            "|1                 |94   |\n",
            "|5                 |21   |\n",
            "+------------------+-----+\n",
            "\n",
            " *** Avg_age の集計結果 ***\n",
            "+-----------+-----+\n",
            "|Avg_age    |count|\n",
            "+-----------+-----+\n",
            "|40-50 years|1028 |\n",
            "|30-40 years|496  |\n",
            "|50-60 years|373  |\n",
            "|60-70 years|64   |\n",
            "|20-30 years|31   |\n",
            "|70-80 years|8    |\n",
            "+-----------+-----+\n",
            "\n",
            " *** Customer_main_type の集計結果 ***\n",
            "+---------------------+-----+\n",
            "|Customer_main_type   |count|\n",
            "+---------------------+-----+\n",
            "|Family with grown ups|542  |\n",
            "|Average Family       |308  |\n",
            "|Conservative families|236  |\n",
            "|Retired and Religious|202  |\n",
            "|Successful hedonists |194  |\n",
            "|Living well          |178  |\n",
            "|Driven Growers       |172  |\n",
            "|Farmers              |93   |\n",
            "|Cruising Seniors     |60   |\n",
            "|Career Loners        |15   |\n",
            "+---------------------+-----+\n",
            "\n",
            " *** label の集計結果 ***\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|0    |1879 |\n",
            "|1    |121  |\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 覚えていますか？ PySpark は Python プログラミングが使えるんです。\n",
        "# つまり、こういうことも Spark の中で出来ます。ただし、 Python はインデントが重要なので注意\n",
        "for col in df.columns:\n",
        "    if col !='Avg_Salary':\n",
        "        print(f\" *** {col} の集計結果 ***\")\n",
        "        df.groupBy(col).count().orderBy('count',ascending=False).show(truncate=False)"
      ]
    },
    {
      "source": [
        "GROUP BY したら、**最大値**とか**最小値**、**平均**などを取りたくなると思います。  \n",
        "これまでの例に出てきたように、COUNT は `count()` を使います。  \n",
        "そのほか、最大値や最小値の取得には、`agg()` を使用していきます。  \n",
        "\n",
        "- **最大値**: `F.max()`\n",
        "- **最小値**: `F.min()`\n",
        "- **合計**: `F.sum()`\n",
        "- **平均**: `F.mean()`\n",
        "\n",
        "F が何か覚えていますか？最初にインポートした関数のやつです。\n",
        "\n",
        "```python\n",
        "import pyspark.sql.functions as F\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+\n|  Customer_main_type|max(Avg_Salary)|\n+--------------------+---------------+\n|             Farmers|          49965|\n|       Career Loners|          49903|\n|Retired and Relig...|          49564|\n|Successful hedonists|       48919896|\n|         Living well|          49816|\n|      Average Family|         991838|\n|    Cruising Seniors|          49526|\n|Conservative fami...|          49965|\n|      Driven Growers|          49932|\n|Family with grown...|          49901|\n+--------------------+---------------+\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_main_type 毎の Avg_Salary の最大値を取得\n",
        "df.groupBy('Customer_main_type').agg(F.max('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+\n|  Customer_main_type|min(Avg_Salary)|\n+--------------------+---------------+\n|             Farmers|          10469|\n|       Career Loners|          13246|\n|Retired and Relig...|           1361|\n|Successful hedonists|          12705|\n|         Living well|          10418|\n|      Average Family|          10506|\n|    Cruising Seniors|          10100|\n|Conservative fami...|          10179|\n|      Driven Growers|          10257|\n|Family with grown...|           1502|\n+--------------------+---------------+\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_main_type 毎の Avg_Salary の最小値を取得\n",
        "df.groupBy('Customer_main_type').agg(F.min('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+\n|  Customer_main_type|sum(Avg_Salary)|\n+--------------------+---------------+\n|             Farmers|        2809468|\n|       Career Loners|         484089|\n|Retired and Relig...|        5522439|\n|Successful hedonists|     3158111161|\n|         Living well|        5552540|\n|      Average Family|       32111040|\n|    Cruising Seniors|        1732220|\n|Conservative fami...|        6963043|\n|      Driven Growers|        5292275|\n|Family with grown...|       15237892|\n+--------------------+---------------+\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_main_type 毎の Avg_Salary の合計値を取得\n",
        "df.groupBy('Customer_main_type').agg(F.sum('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n|  Customer_main_type|     avg(Avg_Salary)|\n+--------------------+--------------------+\n|             Farmers|  30209.333333333332|\n|       Career Loners|             32272.6|\n|Retired and Relig...|   27338.80693069307|\n|Successful hedonists|1.6278923510309279E7|\n|         Living well|  31194.044943820223|\n|      Average Family|  104256.62337662338|\n|    Cruising Seniors|  28870.333333333332|\n|Conservative fami...|  29504.419491525423|\n|      Driven Growers|   30769.04069767442|\n|Family with grown...|  28114.191881918818|\n+--------------------+--------------------+\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_main_type 毎の Avg_Salary の平均値を取得\n",
        "df.groupBy('Customer_main_type').agg(F.mean('Avg_Salary')).show()"
      ]
    },
    {
      "source": [
        "集計関数を使った場合の列名は、`alias()` で変えられます。  \n",
        "RDB で言うところの `AS` XXX ですね。 "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n|  Customer_main_type|          平均給与額|\n+--------------------+--------------------+\n|             Farmers|  30209.333333333332|\n|       Career Loners|             32272.6|\n|Retired and Relig...|   27338.80693069307|\n|Successful hedonists|1.6278923510309279E7|\n|         Living well|  31194.044943820223|\n|      Average Family|  104256.62337662338|\n|    Cruising Seniors|  28870.333333333332|\n|Conservative fami...|  29504.419491525423|\n|      Driven Growers|   30769.04069767442|\n|Family with grown...|  28114.191881918818|\n+--------------------+--------------------+\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_main_type 毎の Avg_Salary の平均値を取得\n",
        "df.groupBy('Customer_main_type').agg(F.mean('Avg_Salary').alias('平均給与額')).show()"
      ]
    },
    {
      "source": [
        "集計したら、**並び替え** もしたくなりますよね。**無論、可能です！**  \n",
        "\bRDB でいう `ORDER BY` 句に紐づくものを使用するには、`sort()` や `orderBy()` を使用します。  \n",
        "ここは、好きなものを使ってください。両方、単一列/複数列で昇順/降順のソートができます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------------------+\n|Customer_subtype                          |mean_salary         |\n+------------------------------------------+--------------------+\n|Low income catholics                      |21713.777777777777  |\n|Single youth                              |24403.25            |\n|Own home elderly                          |25677.666666666668  |\n|Young urban have-nots                     |25751.0             |\n|Lower class large families                |26012.628472222223  |\n|Dinki's (double income no kids)           |26231.117647058825  |\n|Fresh masters in the city                 |27645.0             |\n|Couples with teens 'Married with children'|28155.807228915663  |\n|Residential elderly                       |28866.166666666668  |\n|Mixed small town dwellers                 |28982.106382978724  |\n|Mixed rurals                              |29073.761194029852  |\n|Traditional families                      |29381.84496124031   |\n|Porchless seniors: no front yard          |29509.827586206895  |\n|Young all american family                 |29563.3064516129    |\n|Mixed apartment dwellers                  |29587.647058823528  |\n|Stable family                             |29619.032258064515  |\n|Large religious families                  |29652.196261682242  |\n|Seniors in apartments                     |30090.882352941175  |\n|Young seniors in the city                 |30105.136363636364  |\n|Career and childcare                      |30110.939393939392  |\n|Family starters                           |30376.2             |\n|Religious elderly singles                 |30540.59574468085   |\n|Very Important Provincials                |30548.40625         |\n|Mixed seniors                             |30759.267605633802  |\n|Young and rising                          |30795.897435897437  |\n|Etnically diverse                         |31572.0             |\n|Modern, complete families                 |31576.0             |\n|Middle class families                     |31579.385245901638  |\n|Village families                          |32449.470588235294  |\n|Suburban youth                            |32558.0             |\n|Large family, employed child              |32867.857142857145  |\n|Young, low educated                       |33072.21428571428   |\n|Large family farms                        |33135.61538461538   |\n|Students in apartments                    |35532.142857142855  |\n|Senior cosmopolitans                      |49903.0             |\n|Affluent senior apartments                |653638.8235294118   |\n|Affluent young families                   |662068.7777777778   |\n|High Income, expensive child              |2.3839817807692308E7|\n|High status seniors                       |2.507677857894737E7 |\n+------------------------------------------+--------------------+\n\n"
          ]
        }
      ],
      "source": [
        "# sort() での並び替えの例 - その 1\n",
        "df.groupBy('Customer_subtype').agg(F.avg('Avg_Salary').alias('mean_salary')).sort('mean_salary',ascending=True).show(50,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------+----------+\n|    Customer_subtype|Number_of_houses|    Avg_age|Avg_Salary|\n+--------------------+----------------+-----------+----------+\n|       Mixed seniors|               1|70-80 years|     12065|\n|    Own home elderly|               1|70-80 years|     23340|\n|    Own home elderly|               1|70-80 years|     47001|\n|Seniors in apartm...|               1|70-80 years|     27724|\n|       Mixed seniors|               1|70-80 years|     15518|\n|    Own home elderly|               1|70-80 years|     41473|\n|Lower class large...|               1|70-80 years|     10496|\n|Religious elderly...|               1|70-80 years|     49059|\n|     Family starters|               2|60-70 years|     25224|\n|     Family starters|               2|60-70 years|     40452|\n|Seniors in apartm...|               2|60-70 years|     35243|\n|Dinki's (double i...|               2|60-70 years|     48812|\n|Seniors in apartm...|               2|60-70 years|     28576|\n|     Family starters|               2|60-70 years|     36794|\n|Seniors in apartm...|               2|60-70 years|     29054|\n| Residential elderly|               2|60-70 years|     14801|\n|Seniors in apartm...|               2|60-70 years|     16116|\n|     Family starters|               2|60-70 years|     26154|\n|    Own home elderly|               2|60-70 years|     19039|\n|     Family starters|               2|60-70 years|     11418|\n+--------------------+----------------+-----------+----------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# sort() での並び替えの例 - その 2\n",
        "df.select(['Customer_subtype','Number_of_houses','Avg_age','Avg_Salary']).sort('Avg_age','Number_of_houses',ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------------------+\n|Customer_subtype                          |mean_salary         |\n+------------------------------------------+--------------------+\n|Low income catholics                      |21713.777777777777  |\n|Single youth                              |24403.25            |\n|Own home elderly                          |25677.666666666668  |\n|Young urban have-nots                     |25751.0             |\n|Lower class large families                |26012.628472222223  |\n|Dinki's (double income no kids)           |26231.117647058825  |\n|Fresh masters in the city                 |27645.0             |\n|Couples with teens 'Married with children'|28155.807228915663  |\n|Residential elderly                       |28866.166666666668  |\n|Mixed small town dwellers                 |28982.106382978724  |\n|Mixed rurals                              |29073.761194029852  |\n|Traditional families                      |29381.84496124031   |\n|Porchless seniors: no front yard          |29509.827586206895  |\n|Young all american family                 |29563.3064516129    |\n|Mixed apartment dwellers                  |29587.647058823528  |\n|Stable family                             |29619.032258064515  |\n|Large religious families                  |29652.196261682242  |\n|Seniors in apartments                     |30090.882352941175  |\n|Young seniors in the city                 |30105.136363636364  |\n|Career and childcare                      |30110.939393939392  |\n|Family starters                           |30376.2             |\n|Religious elderly singles                 |30540.59574468085   |\n|Very Important Provincials                |30548.40625         |\n|Mixed seniors                             |30759.267605633802  |\n|Young and rising                          |30795.897435897437  |\n|Etnically diverse                         |31572.0             |\n|Modern, complete families                 |31576.0             |\n|Middle class families                     |31579.385245901638  |\n|Village families                          |32449.470588235294  |\n|Suburban youth                            |32558.0             |\n|Large family, employed child              |32867.857142857145  |\n|Young, low educated                       |33072.21428571428   |\n|Large family farms                        |33135.61538461538   |\n|Students in apartments                    |35532.142857142855  |\n|Senior cosmopolitans                      |49903.0             |\n|Affluent senior apartments                |653638.8235294118   |\n|Affluent young families                   |662068.7777777778   |\n|High Income, expensive child              |2.3839817807692308E7|\n|High status seniors                       |2.507677857894737E7 |\n+------------------------------------------+--------------------+\n\n"
          ]
        }
      ],
      "source": [
        "# orderBy() での並び替えの例 - その 1\n",
        "df.groupBy('Customer_subtype').agg(F.avg('Avg_Salary').alias('mean_salary')).orderBy('mean_salary',ascending=True).show(50,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------+----------+\n|    Customer_subtype|Number_of_houses|    Avg_age|Avg_Salary|\n+--------------------+----------------+-----------+----------+\n|       Mixed seniors|               1|70-80 years|     12065|\n|    Own home elderly|               1|70-80 years|     23340|\n|    Own home elderly|               1|70-80 years|     47001|\n|Seniors in apartm...|               1|70-80 years|     27724|\n|       Mixed seniors|               1|70-80 years|     15518|\n|    Own home elderly|               1|70-80 years|     41473|\n|Lower class large...|               1|70-80 years|     10496|\n|Religious elderly...|               1|70-80 years|     49059|\n|     Family starters|               2|60-70 years|     25224|\n|     Family starters|               2|60-70 years|     40452|\n|Seniors in apartm...|               2|60-70 years|     35243|\n|Dinki's (double i...|               2|60-70 years|     48812|\n|Seniors in apartm...|               2|60-70 years|     28576|\n|     Family starters|               2|60-70 years|     36794|\n|Seniors in apartm...|               2|60-70 years|     29054|\n| Residential elderly|               2|60-70 years|     14801|\n|Seniors in apartm...|               2|60-70 years|     16116|\n|     Family starters|               2|60-70 years|     26154|\n|    Own home elderly|               2|60-70 years|     19039|\n|     Family starters|               2|60-70 years|     11418|\n+--------------------+----------------+-----------+----------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# orderBy() での並び替えの例 - その 2\n",
        "df.select(['Customer_subtype','Number_of_houses','Avg_age','Avg_Salary']).orderBy('Avg_age','Number_of_houses',ascending=False).show()"
      ]
    },
    {
      "source": [
        "## Collect (配列)\n",
        "\n",
        "集計するだけではなく、GROUP BY でグループ分けをしたら、**グループ毎にどんなデータ(要素)が含まれているのか**について確認したくなる時があります。  \n",
        "その時に使用するのが、`Collect` です。  \n",
        "  \n",
        "Collect では、2 種類の配列を返すことができます。  \n",
        "\n",
        "- `F.collect_list()`: すべての要素を元のデータセットのデータ順で表示\n",
        "- `F.collect_set()`: 一意の値のみを表示 (いわゆる RDB でいう **DISTINCT**)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------------------+\n|    Customer_subtype|collect_list(Number_of_houses)|\n+--------------------+------------------------------+\n|Large family, emp...|          [2, 1, 2, 1, 2, 1...|\n|Religious elderly...|          [1, 1, 1, 1, 1, 1...|\n|Large religious f...|          [2, 1, 1, 2, 1, 1...|\n|Modern, complete ...|          [1, 1, 2, 1, 1, 1...|\n|    Village families|          [1, 1, 1, 1, 1, 1...|\n|Young all america...|          [1, 1, 2, 2, 1, 1...|\n|Young urban have-...|                  [1, 2, 1, 1]|\n|Young seniors in ...|          [1, 1, 1, 1, 1, 2...|\n|Fresh masters in ...|                        [1, 1]|\n|High Income, expe...|          [1, 1, 1, 1, 1, 1...|\n|Lower class large...|          [1, 1, 1, 1, 1, 1...|\n| Residential elderly|            [3, 1, 1, 3, 2, 1]|\n|Senior cosmopolitans|                           [3]|\n|        Mixed rurals|          [1, 1, 1, 1, 1, 1...|\n|Career and childcare|          [2, 1, 1, 1, 1, 1...|\n|Low income catholics|          [1, 1, 1, 1, 1, 1...|\n|Mixed apartment d...|          [2, 3, 1, 1, 1, 1...|\n|Seniors in apartm...|          [2, 2, 2, 2, 1, 2...|\n|Middle class fami...|          [1, 1, 2, 1, 1, 1...|\n|Traditional families|          [1, 1, 1, 1, 1, 1...|\n+--------------------+------------------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_subtype でグループ化し、Number_of_houses の collect_list を取得\n",
        "df.groupby(\"Customer_subtype\").agg(F.collect_list(\"Number_of_houses\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------------------+\n|    Customer_subtype|collect_set(Number_of_houses)|\n+--------------------+-----------------------------+\n|Large family, emp...|                       [1, 2]|\n|Religious elderly...|                       [1, 2]|\n|Large religious f...|                       [1, 2]|\n|Modern, complete ...|                       [1, 2]|\n|    Village families|                       [1, 2]|\n|Young all america...|                       [1, 2]|\n|Young urban have-...|                       [1, 2]|\n|Young seniors in ...|                    [1, 2, 3]|\n|Fresh masters in ...|                          [1]|\n|High Income, expe...|                          [1]|\n|Lower class large...|                       [1, 2]|\n| Residential elderly|                    [1, 2, 3]|\n|Senior cosmopolitans|                          [3]|\n|        Mixed rurals|                          [1]|\n|Career and childcare|                       [1, 2]|\n|Low income catholics|                          [1]|\n|Mixed apartment d...|                    [1, 2, 3]|\n|Seniors in apartm...|                       [1, 2]|\n|Middle class fami...|                       [1, 2]|\n|Traditional families|                       [1, 2]|\n+--------------------+-----------------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# Customer_subtype でグループ化し、Number_of_houses の collect_set を取得\n",
        "df.groupby(\"Customer_subtype\").agg(F.collect_set(\"Number_of_houses\")).show()"
      ]
    },
    {
      "source": [
        "## User-Defined Functions (ユーザー定義関数)\n",
        "\n",
        "RDB でいう`ユーザー定義関数`と同じことは PySpark でも実現可能です。  \n",
        "特定のデータ処理をユーザー定義関数として用意しておくと、定義するだけでいろんな場所で流用できるため、非常に便利です。  \n",
        "やり方は、`Python で関数を定義する`方法と同じです。(だって、ユーザー定義「関数」だもの)  \n",
        "ただし、気をつける点として、**使用する際は関数の戻り値のデータ型について定義** してあげる必要があります。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ユーザー定義関数 (UDF) を使用するためのライブラリをインポート\n",
        "from pyspark.sql.functions import udf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 年齢毎にでカテゴリを割り当てる UDF を作成\n",
        "def age_category(age):\n",
        "    if age  == '20-30 years':\n",
        "        return 'Young'\n",
        "    elif age== '30-40 years':\n",
        "        return 'Mid Aged' \n",
        "    elif ((age== '40-50 years') or (age== '50-60 years')) :\n",
        "        return 'Old'\n",
        "    else:\n",
        "        return 'Very Old'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UDF の戻り値のデータ型を設定 (今回は string: 文字列)\n",
        "age_udf = udf(age_category,StringType())"
      ]
    },
    {
      "source": [
        "PySpark では、`バケット列 (bucket column)` を作成することができます。  \n",
        "これは、主にグループ分けをする際に用いられる列で、新たに数式やカスタム項目を追加する必要がなくなるため、便利です。  \n",
        "`withColumn()` で指定します。(まあ、いわゆる Excel 関数使用のための列追加みたいなものです)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+------------+\n|    Customer_subtype|    Avg_age|age_category|\n+--------------------+-----------+------------+\n|Lower class large...|30-40 years|    Mid Aged|\n|Mixed small town ...|30-40 years|    Mid Aged|\n|Mixed small town ...|30-40 years|    Mid Aged|\n|Modern, complete ...|40-50 years|         Old|\n|  Large family farms|30-40 years|    Mid Aged|\n|    Young and rising|20-30 years|       Young|\n|Large religious f...|30-40 years|    Mid Aged|\n|Lower class large...|40-50 years|         Old|\n|Lower class large...|50-60 years|         Old|\n|     Family starters|40-50 years|         Old|\n|       Stable family|40-50 years|         Old|\n|Modern, complete ...|40-50 years|         Old|\n|Lower class large...|40-50 years|         Old|\n|        Mixed rurals|40-50 years|         Old|\n|    Young and rising|30-40 years|    Mid Aged|\n|Lower class large...|40-50 years|         Old|\n|Traditional families|40-50 years|         Old|\n|Mixed apartment d...|40-50 years|         Old|\n|Young all america...|30-40 years|    Mid Aged|\n|Low income catholics|50-60 years|         Old|\n+--------------------+-----------+------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# バケット列として age_category を用意し、値に UDF を実行結果を入れる\n",
        "df = df.withColumn('age_category',age_udf(df['Avg_age']))\n",
        "df.select('Customer_subtype','Avg_age','age_category').show()"
      ]
    },
    {
      "source": [
        "もちろん、バケット列も GROUP BY したりすることが可能です。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n|age_category|count|\n+------------+-----+\n|    Mid Aged|  496|\n|    Very Old|   72|\n|         Old| 1401|\n|       Young|   31|\n+------------+-----+\n\n"
          ]
        }
      ],
      "source": [
        "# バケット列を groupBy() して count() を実行\n",
        "df.groupBy(\"age_category\").count().show()"
      ]
    },
    {
      "source": [
        "### Pandas UDF (pandas を使ったユーザー定義関数)\n",
        "\n",
        "Python には、**データ分析用のライブラリ: Pandas** というとても有名なものがあります。  \n",
        "Pandas は、ビックデータなどの大きな表形式データを扱うことができる、とても便利なライブラリです。  \n",
        "\n",
        "PySpark を使ってデータ分析を行う際の大きなメリットと言っても過言ではないと思います。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "#### 標準の Python UDF と Pandas UDF の違い\n",
        "\n",
        "大きな違いは、標準の Python UDF と比べて、`Pandas UDF のほうが処理時間と実行時間の点で遥かに高速で効率的`に動作するという点です。  \n",
        "標準の Python UDF は、_行ごと_に UDF を実行します。そのため、分散プレームワークの利点を実際には活用できていません。  \n",
        "Pandas UDF は、`ブロックごと`で処理を実行するため、分散フレームワークの利点を活かし、高速処理を実現しています。  \n",
        "`ただし、データフレームが大きいと、かなりメモリを喰います！メモリ節約のコツ的なものを一緒に覚えておくと良いです。`(今回は時間の関係上、割愛します)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pandas UDF を使用するためのライブラリをインポート\n",
        "import pandas as pd\n",
        "#from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.functions import pandas_udf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- Customer_subtype: string (nullable = true)\n |-- Number_of_houses: integer (nullable = true)\n |-- Avg_size_household: integer (nullable = true)\n |-- Avg_age: string (nullable = true)\n |-- Customer_main_type: string (nullable = true)\n |-- Avg_Salary: integer (nullable = true)\n |-- label: integer (nullable = true)\n |-- age_category: string (nullable = true)\n\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_sal=1361\n",
        "max_sal=48919896"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pandas UDF を作成\n",
        "def scaled_salary(salary):\n",
        "    scaled_sal=(salary-min_sal)/(max_sal-min_sal)\n",
        "    return scaled_sal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pandas UDF の戻り値のデータ型を設定 (今回は double: 浮動小数点)\n",
        "scaling_udf = pandas_udf(scaled_salary, DoubleType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o473.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 143.0 failed 1 times, most recent failure: Lost task 0.0 in stage 143.0 (TID 3289, 9f7e57e3df08, executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor48.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-cb35e3c9c403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaled_salary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Avg_Salary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o473.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 143.0 failed 1 times, most recent failure: Lost task 0.0 in stage 143.0 (TID 3289, 9f7e57e3df08, executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor48.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"scaled_salary\", scaling_udf(df['Avg_Salary'])).show(10,False)"
      ]
    },
    {
      "source": [
        "## Joins (結合)\n",
        "\n",
        "データセット同士は、マージすることができます。  \n",
        "`join()`を使用します。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n|  Customer_main_type|Region Code|\n+--------------------+-----------+\n|Family with grown...|         PN|\n|      Driven Growers|         GJ|\n|Conservative fami...|         DD|\n|    Cruising Seniors|         DL|\n|     Average Family |         MN|\n|         Living well|         KA|\n|Successful hedonists|         JH|\n|Retired and Relig...|         AX|\n|       Career Loners|         HY|\n|             Farmers|         JH|\n+--------------------+-----------+\n\n"
          ]
        }
      ],
      "source": [
        "# マージ用のデータフレームを作成\n",
        "region_data = spark.createDataFrame([('Family with grown ups','PN'),\n",
        "                                    ('Driven Growers','GJ'),\n",
        "                                    ('Conservative families','DD'),\n",
        "                                    ('Cruising Seniors','DL'),\n",
        "                                    ('Average Family ','MN'),\n",
        "                                    ('Living well','KA'),\n",
        "                                    ('Successful hedonists','JH'),\n",
        "                                    ('Retired and Religious','AX'),\n",
        "                                   ('Career Loners','HY'),('Farmers','JH')],schema=StructType().add(\"Customer_main_type\",\"string\").add(\"Region Code\",\"string\"))\n",
        "region_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------------+------------------+-----------+----------+-----+-----------+\n|  Customer_main_type|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|Avg_Salary|label|Region Code|\n+--------------------+--------------------+----------------+------------------+-----------+----------+-----+-----------+\n|Family with grown...|Lower class large...|               1|                 2|40-50 years|     25596|    0|         PN|\n|Family with grown...|Mixed small town ...|               1|                 2|40-50 years|     26579|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 4|30-40 years|     33537|    0|         PN|\n|Family with grown...|    Village families|               1|                 3|40-50 years|     22089|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 2|40-50 years|     38712|    0|         PN|\n|Family with grown...|    Village families|               1|                 2|50-60 years|     21519|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 3|40-50 years|     43214|    0|         PN|\n|Family with grown...|Couples with teen...|               1|                 2|50-60 years|     45028|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 3|40-50 years|     25747|    0|         PN|\n|Family with grown...|Couples with teen...|               1|                 2|50-60 years|     42847|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 2|50-60 years|     31415|    0|         PN|\n|Family with grown...|    Village families|               1|                 2|40-50 years|     17451|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 3|40-50 years|     29011|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 2|40-50 years|     42372|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 2|50-60 years|     34910|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 3|40-50 years|     10707|    1|         PN|\n|Family with grown...|Lower class large...|               2|                 3|40-50 years|     19189|    0|         PN|\n|Family with grown...|Lower class large...|               2|                 3|40-50 years|      5306|    0|         PN|\n|Family with grown...|Couples with teen...|               1|                 2|40-50 years|     46423|    0|         PN|\n|Family with grown...|Lower class large...|               1|                 3|40-50 years|     24911|    0|         PN|\n+--------------------+--------------------+----------------+------------------+-----------+----------+-----+-----------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# 既存のデータフレームに作成したマージ用のデータフレームを結合\n",
        "# キーは Customer_main_type\n",
        "new_df=df.join(region_data,on='Customer_main_type')\n",
        "new_df.show()"
      ]
    },
    {
      "source": [
        "## Pivoting (ピボット)\n",
        "\n",
        "いわゆる `ピボットテーブル`を作れます。  \n",
        "データフレームを作成して、分析や確認のために別の形に表を切り替えたい時などに使えます。  \n",
        "例えは、  \n",
        "\n",
        "|Product|Amount|Country|  \n",
        "| :-- | :-- | :-- |  \n",
        "|Banana |1000  |USA    |  \n",
        "|Carrots|1500  |USA    |  \n",
        "|Beans  |1600  |USA    |  \n",
        "|Orange |2000  |USA    |  \n",
        "|Orange |2000  |USA    |  \n",
        "|Banana |400   |China  |  \n",
        "|Carrots|1200  |China  |  \n",
        "|Beans  |1500  |China  |  \n",
        "|Orange |4000  |China  |  \n",
        "|Banana |2000  |Canada |  \n",
        "|Carrots|2000  |Canada |  \n",
        "|Beans  |2000  |Mexico |  \n",
        "\n",
        "こういうデータフレームがあったとして、これを  \n",
        "\n",
        "| Product | Canada | China | Mexico | USA |\n",
        "| :-- | :-- | :-- | :-- | :-- |  \n",
        "|Orange |null  |4000 |null  |4000|  \n",
        "|Beans  |null  |1500 |2000  |1600|  \n",
        "|Banana |2000  |400  |null  |1000|  \n",
        "|Carrots|2000  |1200 |null  |1500|  \n",
        "\n",
        "のように、Country の列を回転させて、Product 列でグルーピングした値を並べて表示、みたいなことができます。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n|  Customer_main_type|20-30 years|30-40 years|40-50 years|50-60 years|60-70 years|70-80 years|\n+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n|             Farmers|          0|     462027|    2031235|     316206|          0|          0|\n|       Career Loners|     143998|     176639|      25701|     105193|      32558|          0|\n|Retired and Relig...|     126350|     336631|    2975266|    1687711|     335357|      61124|\n|Successful hedonists|      42261|  171278764| 1223362814| 1563071675|  200340129|      15518|\n|         Living well|     460528|    2965303|    1795405|     331304|          0|          0|\n|      Average Family|          0|   23682805|    7789464|     412490|     226281|          0|\n|    Cruising Seniors|          0|      43302|     303601|     529354|     716425|     139538|\n|Conservative fami...|      69390|    2381485|    3449782|     915954|     146432|          0|\n|      Driven Growers|          0|    1376260|    3407807|     424272|      83936|          0|\n|Family with grown...|      16406|    2620378|    9132414|    3295378|     162820|      10496|\n+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n\n"
          ]
        }
      ],
      "source": [
        "# データフレームを Customer_main_type でグループ化し、Avg_age でピボット\n",
        "df.groupBy('Customer_main_type').pivot('Avg_age').sum('Avg_salary').fillna(0).show()"
      ]
    },
    {
      "source": [
        "ピボットは、データの分析を行う際にとても便利です。  \n",
        "`Excel で頑張って同じものを作ろうとすると、生産性が低下し、無駄につながります。`  \n",
        "この機会に、覚えておくと良いと思います。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Window Functions or Window Aggregates (ウインドウ関数またはウインドウ集計)\n",
        "\n",
        "RDB の`ウインドウ関数`に該当するものです。  \n",
        "PySpark では、以下の 3 種類のウインドウ関数がサポートされています。  \n",
        "\n",
        "- Aggregations (集合)\n",
        "- Ranking (順位付け)\n",
        "- Analytics (分析)\n",
        "\n",
        "以下のライブラリのインポートは忘れないようにしてください。\n",
        "\n",
        "```python\n",
        "from pyspark.sql.window import Window\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 今回は、Ranking を試してみます\n",
        "# 実行に必要なライブラリをインポート\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import udf,rank, col,row_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avg Salary を順番に並べるためのウィンドウ関数を作成\n",
        "win = Window.orderBy(df['Avg_Salary'].desc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----------+----------+----+\n|  Customer_main_type|    Customer_subtype|    Avg_age|Avg_Salary|rank|\n+--------------------+--------------------+-----------+----------+----+\n|Successful hedonists| High status seniors|60-70 years|  48919896|   1|\n|Successful hedonists|High Income, expe...|50-60 years|  48177970|   2|\n|Successful hedonists|High Income, expe...|50-60 years|  48069548|   3|\n|Successful hedonists|High Income, expe...|40-50 years|  46911924|   4|\n|Successful hedonists| High status seniors|40-50 years|  46614009|   5|\n|Successful hedonists|High Income, expe...|30-40 years|  45952441|   6|\n|Successful hedonists|High Income, expe...|40-50 years|  45864609|   7|\n|Successful hedonists| High status seniors|50-60 years|  45592572|   8|\n|Successful hedonists| High status seniors|50-60 years|  45170899|   9|\n|Successful hedonists|High Income, expe...|50-60 years|  44843830|  10|\n|Successful hedonists| High status seniors|50-60 years|  43230349|  11|\n|Successful hedonists| High status seniors|40-50 years|  43181830|  12|\n|Successful hedonists| High status seniors|30-40 years|  42631926|  13|\n|Successful hedonists| High status seniors|50-60 years|  41919020|  14|\n|Successful hedonists| High status seniors|50-60 years|  41699271|  15|\n|Successful hedonists| High status seniors|40-50 years|  41398953|  16|\n|Successful hedonists|High Income, expe...|40-50 years|  41269615|  17|\n|Successful hedonists| High status seniors|50-60 years|  41192397|  18|\n|Successful hedonists| High status seniors|40-50 years|  40564335|  19|\n|Successful hedonists| High status seniors|50-60 years|  40453887|  20|\n+--------------------+--------------------+-----------+----------+----+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "source": [
        "# 行番号を rank としてバケット列を追加\n",
        "df = df.withColumn('rank', row_number().over(win).alias('rank'))\n",
        "df.select(['Customer_main_type','Customer_subtype','Avg_age','Avg_Salary','rank']).sort('rank').show()"
      ]
    },
    {
      "source": [
        "# まとめ\n",
        "\n",
        "今回は、PySpark Intro ということで、`PySpark でどういったデータ操作が可能なのか`について紹介しました。  \n",
        "**Spark ムズカシイ、PySpark 何それ美味しいの？** と思っていたみなさん、SQL の知識があれば、`あれ、PySpark 意外と簡単じゃね？`と思ってもらえるのではないかと思います。  \n",
        "\n",
        "今回ご紹介した\n",
        "\n",
        "- Dataframes (データフレーム)\n",
        "- Select (選択)\n",
        "- Filter (フィルタリング)\n",
        "- Where (条件指定)\n",
        "- Aggregations (集合)\n",
        "- Collect (配列)\n",
        "- User-Defined Functions (ユーザー定義関数)\n",
        "- Joins (結合)\n",
        "- Pivoting (ピボット)\n",
        "- Window Functions or Window Aggregates (ウインドウ関数またはウインドウ集計)\n",
        "\n",
        "を振り返ってもらって、是非、Azure Synapse Analytics の Spark プールの活用を始めてみてください。 \n",
        "SQL on-demand もいいけど、やっぱりデータ分析の先にあるのは、**どの目的でデータを活用するのか** です。その手段の 1 つとして機械学習があると思いますが、`機械学習をするなら Python/Spark の方が何かと好都合です。`  \n",
        "是非、自分の会社の中に埋もれている、いろんなデータを PySpark を使って分析してみてください。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# 参考情報\n",
        "\n",
        "## Microsoft Docs\n",
        "\n",
        "- [Azure Synapse Analytics での Apache Spark](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/spark/apache-spark-overview)\n",
        "\n",
        "## Spark SQL Guide\n",
        "\n",
        "- [PySpark Usage Guide for Pandas with Apache Arrow](http://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)\n",
        "\n",
        "## @IT\n",
        "\n",
        "- [Apache Sparkとは何か――使い方や基礎知識を徹底解説 ](https://www.atmarkit.co.jp/ait/articles/1608/24/news014.html)\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}